# ğŸ¤– U-Ask UAE Government Chatbot - AI/ML QA Automation Framework

[![Playwright Tests](https://img.shields.io/badge/Playwright-Tests-green.svg)](https://playwright.dev/)
[![DeepEval](https://img.shields.io/badge/DeepEval-AI%20Metrics-blue.svg)](https://deepeval.com/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

> **Professional End-to-End Automated Testing Framework for AI-Powered Chatbot**  
> *Comprehensive UI, AI Response, and Security Validation with Industry-Standard Reporting*

---

## ğŸ“‹ Table of Contents
- [Overview](#overview)
- [Test Coverage](#test-coverage)
- [Technology Stack](#technology-stack)
- [Quick Start](#quick-start)
- [Test Execution](#test-execution)
- [Reporting](#reporting)
- [Project Structure](#project-structure)
- [CI/CD Integration](#cicd-integration)

---

## ğŸ¯ Overview

This framework provides comprehensive automated testing for the **U-Ask UAE Government Chatbot**, covering:

- âœ… **UI Behavior Validation** - Widget loading, interactions, responsiveness
- âœ… **AI Response Quality** - Hallucination detection, relevancy, consistency
- âœ… **Security Testing** - XSS, SQL injection, prompt injection
- âœ… **Multilingual Support** - English (LTR) and Arabic (RTL) testing
- âœ… **Cross-Browser Testing** - Chrome, Firefox, Safari
- âœ… **Mobile Responsiveness** - iPhone, Android devices
- âœ… **AI/ML Metrics** - 12 DeepEval metrics for comprehensive AI evaluation

**Total Test Coverage: 27 Playwright Tests + 12 DeepEval Metrics**

---

## ğŸ“Š Test Coverage

### Playwright Test Suite (27 Tests)

| Test Category | Count | Coverage |
|--------------|-------|----------|
| **UI Behavior** | 8 | Chat widget, input/output, loading states, accessibility |
| **AI Response Validation** | 10 | Relevancy, hallucination, consistency, quality metrics |
| **Security & Injection** | 5 | XSS, SQL injection, prompt injection, jailbreak attempts |
| **Cross-Browser & Mobile** | 4 | Chrome, Firefox, iPhone, Android responsiveness |

### DeepEval AI Metrics (12 Metrics)

| Metric | Purpose |
|--------|---------|
| **Hallucination Detection** | Ensures no fabricated information |
| **Answer Relevancy** | Validates response addresses query |
| **Faithfulness** | Verifies grounding in context |
| **Contextual Precision** | Measures context ranking quality |
| **Contextual Recall** | Ensures full context utilization |
| **Contextual Relevancy** | Validates context appropriateness |
| **Bias Detection** | Ensures neutral responses |
| **Toxicity Check** | Validates respectful communication |
| **Helpfulness (G-Eval)** | Custom metric for actionability |
| **Completeness (G-Eval)** | Validates comprehensive answers |
| **Clarity (G-Eval)** | Ensures clear communication |
| **Accuracy (G-Eval)** | Verifies factual correctness |

---

## ğŸ›  Technology Stack
```
Playwright v1.42       - E2E testing framework
DeepEval v0.21        - AI/ML evaluation metrics
Allure v2.27          - Professional test reporting
Node.js v18+          - Runtime environment
Python 3.9+           - DeepEval execution
```

---

## ğŸš€ Quick Start

### Prerequisites
```bash
node --version  # v18.0.0 or higher
python --version  # 3.9 or higher
```

### Installation
```bash
# Clone the repository
git clone https://github.com/yourusername/u-ask-automation.git
cd u-ask-automation

# Install Node dependencies
npm install

# Install Playwright browsers
npx playwright install

# Install Python dependencies
pip install -r requirements.txt

# Login to DeepEval (for Confident AI dashboard)
deepeval login
```

### Environment Setup

Create `.env` file:
```env
# OpenAI API Key for DeepEval metrics
OPENAI_API_KEY=your_openai_api_key_here

# Test Configuration
BASE_URL=https://ask.u.ae
TEST_TIMEOUT=60000
```

---

## â–¶ï¸ Test Execution

### Run All Tests
```bash
npm test
```

### Run Specific Test Suites
```bash
# UI tests only
npm run test:ui

# AI validation tests
npm run test:ai-validation

# Security tests
npm run test:security

# Cross-browser tests
npm run test:chrome
npm run test:firefox
npm run test:mobile
```

### Run with UI Mode (Visual Debugging)
```bash
npm run test:ui
```

### Run DeepEval AI Metrics
```bash
# Run all DeepEval tests
npm run deepeval

# Run specific metrics
cd ai-evaluation
deepeval test run deepeval-tests.py::test_hallucination_metric
```

### Run Complete Test Suite (Playwright + DeepEval)
```bash
npm run test:all
```

---

## ğŸ“ˆ Reporting

### Playwright HTML Report
```bash
# Generate and open HTML report
npm run report
```
Report location: `reports/playwright-html/index.html`

### Allure Report (Professional)
```bash
# Generate Allure report
npm run allure:generate

# Open Allure report in browser
npm run allure:open
```
Report location: `allure-report/index.html`

**Allure Features:**
- âœ… Test execution timeline
- âœ… Test categories (UI, AI, Security)
- âœ… Screenshot attachments on failure
- âœ… Video recordings
- âœ… Trends and history
- âœ… Suites and behaviors view

### DeepEval Confident AI Dashboard
```bash
# Login and view results online
deepeval login

# Tests automatically sync to dashboard
# Visit: https://app.confident-ai.com
```

**Confident AI Features:**
- âœ… Real-time metric visualization
- âœ… Test case management
- âœ… Regression tracking
- âœ… AI model comparison
- âœ… Team collaboration

---

## ğŸ“ Project Structure
```
u-ask-chatbot-automation/
â”œâ”€â”€ pages/                          # Page Object Models
â”‚   â”œâ”€â”€ BasePage.js
â”‚   â””â”€â”€ ChatbotPage.js
â”œâ”€â”€ tests/                          # Test Specifications
â”‚   â”œâ”€â”€ ui-behavior.spec.js         (8 tests)
â”‚   â”œâ”€â”€ ai-response-validation.spec.js  (10 tests)
â”‚   â”œâ”€â”€ security-injection.spec.js  (5 tests)
â”‚   â””â”€â”€ cross-browser-mobile.spec.js  (4 tests)
â”œâ”€â”€ utils/                          # Helper Utilities
â”‚   â”œâ”€â”€ aiValidator.js
â”‚   â””â”€â”€ testHelpers.js
â”œâ”€â”€ ai-evaluation/                  # DeepEval AI Metrics
â”‚   â”œâ”€â”€ deepeval-tests.py           (12 metrics)
â”‚   â”œâ”€â”€ test-cases.json
â”‚   â””â”€â”€ playwright-results.json
â”œâ”€â”€ test-data/                      # Test Data
â”‚   â”œâ”€â”€ prompts-en.json
â”‚   â”œâ”€â”€ prompts-ar.json
â”‚   â””â”€â”€ injection-tests.json
â”œâ”€â”€ reports/                        # Test Reports
â”‚   â”œâ”€â”€ playwright-html/
â”‚   â”œâ”€â”€ screenshots/
â”‚   â”œâ”€â”€ videos/
â”‚   â””â”€â”€ test-results.json
â”œâ”€â”€ allure-results/                 # Allure Raw Results
â”œâ”€â”€ allure-report/                  # Allure HTML Report
â”œâ”€â”€ playwright.config.js            # Playwright Configuration
â”œâ”€â”€ pytest.ini                      # Pytest Configuration
â”œâ”€â”€ package.json                    # Node Dependencies
â”œâ”€â”€ requirements.txt                # Python Dependencies
â”œâ”€â”€ .env                            # Environment Variables
â””â”€â”€ README.md                       # Documentation
```

---

## ğŸ”„ CI/CD Integration

### GitHub Actions Workflow
```yaml
name: U-Ask Chatbot Tests

on: [push, pull_request]

jobs:
  playwright-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-node@v3
        with:
          node-version: 18
      - run: npm install
      - run: npx playwright install --with-deps
      - run: npm test
      - uses: actions/upload-artifact@v3
        if: always()
        with:
          name: allure-results
          path: allure-results/
  
  deepeval-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - run: pip install -r requirements.txt
      - run: cd ai-evaluation && deepeval test run deepeval-tests.py
```

---

## ğŸ“ Test Summary

### Execution Metrics
- **Total Tests**: 27 Playwright + 12 DeepEval = **39 Total Tests**
- **Estimated Execution Time**: 15-20 minutes
- **Coverage**: UI (30%), AI Validation (37%), Security (19%), Cross-Browser (14%)

### Quality Gates
- âœ… All UI interactions must work flawlessly
- âœ… AI responses must score > 0.7 on all metrics
- âœ… Zero security vulnerabilities tolerated
- âœ… Cross-browser compatibility verified
- âœ… Mobile responsiveness confirmed

---

## ğŸ‘¤ Author

**Shilpa** - Senior QA Automation Engineer  
*Specialization: AI/ML Testing, Playwright Automation, Test Strategy*

---

## ğŸ“„ License

MIT License - See LICENSE file for details

---

## ğŸ¤ Contributing

This is an assessment project. For inquiries, please contact the author.

---

**Last Updated**: December 2024  
**Framework Version**: 1.0.0  
**Playwright Version**: 1.42.0  
**DeepEval Version**: 0.21.73

//-------------------------------------------------------------------------------------------------------

U-Ask UAE Government Chatbot â€“ AI QA Automation Framework
ğŸ“Œ Overview

This repository contains an end-to-end AI QA automation framework for the U-Ask UAE Government Chatbot, built using:

Playwright (UI automation)

Promptfoo (LLM/AI response evaluation)

JavaScript-based assertions

Robust auto-waiting logic (no static sleeps)

The framework validates UI behavior, AI response quality, and content reliability under real user conditions.

ğŸ§  Architecture Summary

u-ask-automation/
â”‚
â”œâ”€â”€ providers/
â”‚   â””â”€â”€ uask-chatbot-provider.js   # Playwright-based Promptfoo provider
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ 1_basic/
â”‚   â”‚   â””â”€â”€ basic_test.yaml        # Core AI response checks
â”‚   â”œâ”€â”€ 2_moderate/
â”‚   â”‚   â””â”€â”€ moderate_test.yaml     # Language, fallback, response quality
â”‚   â””â”€â”€ 3_advanced/
â”‚       â””â”€â”€ advanced_test.yaml     # Complex & multi-topic queries
â”‚
â”œâ”€â”€ promptfoo/
â”‚   â”œâ”€â”€ promptfooconfig.yaml
â”‚   â”œâ”€â”€ promptfooconfig-moderate.yaml
â”‚   â””â”€â”€ promptfooconfig-advanced.yaml
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ package.json
â””â”€â”€ README.md
âš™ï¸ Key Design Decisions (Latest Updates)
âœ… Robust Auto-Waiting (Provider Fix)

The Playwright provider waits only for a real, final AI response, ensuring:

Loading / placeholder text is gone

Response is stable (not streaming)

No system or retry errors

Meaningful length is reached

âŒ No waitForTimeout()
âœ… Uses expect.poll() + stability checks

âœ… Assertion Reliability Fixes

All Promptfoo JavaScript assertions now:

Explicitly return true / false

Avoid false negatives caused by:

Missing returns

Over-strict formatting rules

Legitimate usage of the word â€œerrorâ€ in normal advice

âœ… Concurrency & Timeout Stability

Concurrency forced to 1

Timeout standardized to 180s

Prevents flaky UI behavior and browser closure issues

âœ… Coverage Matrix (What is Covered)
ğŸŸ¢ Functional & AI Quality Coverage
Area	Covered
Clear & helpful responses	âœ…
Clean formatting (steps, bullets, sections)	âœ…
Placeholder / loading detection	âœ…
Fallback handling (out-of-scope questions)	âœ…
Arabic language response	âœ…
English response quality	âœ…
Contextual relevance	âœ…
Complex & multi-topic queries	âœ…
Error / retry message detection	âœ…
ğŸŸ¢ Test Levels
1ï¸âƒ£ Basic Tests

Government services

License renewal

Visa documents

Business registration

Step-by-step validation

2ï¸âƒ£ Moderate Tests

Fallback behavior

Arabic responses

General response quality

Non-refusal, non-hallucinated answers

3ï¸âƒ£ Advanced Tests

New resident onboarding

Trade license documentation

Multi-topic queries
(Visa + School enrollment + Health insurance)

âš ï¸ Intentionally Not Covered (Yet)

These are explicitly out of scope for the current implementation and documented intentionally:

Area	Status	Reason
True hallucination detection	âŒ	Requires factual verification or external data sources
Answer consistency across runs	âŒ	LLM responses are probabilistic
Multi-turn conversational memory	âŒ	Promptfoo is single-turn by default
Source citation enforcement	âŒ	Not supported by chatbot UI
Token-level streaming validation	âŒ	UI abstracts streaming chunks

ğŸ” Note: Hallucination checks are approximated via relevance & structure, not factual truth.

â–¶ï¸ How to Run Tests
Install dependencies
npm install

Run Basic Tests
npm run promptfoo:basic

Run Moderate Tests
npm run promptfoo:moderate

Run Advanced Tests
npm run promptfoo:advanced

ğŸ§ª Evaluation Philosophy

This framework is designed to answer real QA questions:

â€œIs the chatbot behaving correctly for an end user?â€

Rather than verifying exact answers, it validates:

Completeness

Relevance

Language correctness

UX stability

Error-free responses

This mirrors production AI QA best practices.

Future Enhancements (Optional)

Multi-turn context validation

Hallucination detection using reference datasets

Source citation enforcement

Performance & latency benchmarks

CI/CD integration (GitHub Actions)

ğŸ‘©â€ğŸ’» Author

Shilpa Saware
Senior QA 
